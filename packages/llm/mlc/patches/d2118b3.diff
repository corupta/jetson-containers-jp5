Submodule 3rdparty/tvm contains modified content
Submodule 3rdparty/cutlass contains modified content
diff --git a/3rdparty/tvm/3rdparty/cutlass/CMakeLists.txt b/3rdparty/tvm/3rdparty/cutlass/CMakeLists.txt
index ed759073..2126a44e 100755
--- a/3rdparty/tvm/3rdparty/cutlass/CMakeLists.txt
+++ b/3rdparty/tvm/3rdparty/cutlass/CMakeLists.txt
@@ -138,13 +138,7 @@ set(CUTLASS_ENABLE_GTEST_UNIT_TESTS ${CUTLASS_ENABLE_TESTS} CACHE BOOL "Enable C
 
 set(CUTLASS_NVCC_ARCHS_SUPPORTED "")
 if (CUDA_VERSION VERSION_GREATER_EQUAL 11.4 AND NOT CUDA_COMPILER MATCHES "[Cc]lang")
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 70 72 75 80 86 87)
-endif()
-if (CUDA_VERSION VERSION_GREATER_EQUAL 11.8 AND NOT CUDA_COMPILER MATCHES "[Cc]lang")
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 89 90)
-endif()
-if (CUDA_VERSION VERSION_GREATER_EQUAL 12.0 AND NOT CUDA_COMPILER MATCHES "[Cc]lang")
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 90a)
+   list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 72 87)
 endif()
 set(CUTLASS_NVCC_ARCHS ${CUTLASS_NVCC_ARCHS_SUPPORTED} CACHE STRING "The SM architectures requested.")
 set(CUTLASS_NVCC_ARCHS_ENABLED ${CUTLASS_NVCC_ARCHS} CACHE STRING "The SM architectures to build code for.")
Submodule 3rdparty/cutlass_fpA_intB_gemm contains modified content
Submodule cutlass contains modified content
diff --git a/3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm/cutlass/CMakeLists.txt b/3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm/cutlass/CMakeLists.txt
index 30e261c2..bb5f2427 100755
--- a/3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm/cutlass/CMakeLists.txt
+++ b/3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm/cutlass/CMakeLists.txt
@@ -101,26 +101,8 @@ if (CUTLASS_ENABLE_TESTS)
 endif()
 
 set(CUTLASS_NVCC_ARCHS_SUPPORTED "")
-if (NOT CUDA_VERSION VERSION_LESS 7.5)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 53)
-endif()
-if (NOT CUDA_VERSION VERSION_LESS 8.0)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 60 61)
-endif()
-if (NOT CUDA_VERSION VERSION_LESS 9.0)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 70)
-endif()
-if (NOT CUDA_VERSION VERSION_LESS 9.2)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 72)
-endif()
-if (NOT CUDA_VERSION VERSION_LESS 10.0)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 75)
-endif()
-if (NOT CUDA_VERSION VERSION_LESS 11.0)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 80)
-endif()
-if (NOT CUDA_VERSION VERSION_LESS 11.1 AND NOT CUDA_COMPILER MATCHES "[Cc]lang")
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 86)
+if (CUDA_VERSION VERSION_GREATER_EQUAL 11.4 AND NOT CUDA_COMPILER MATCHES "[Cc]lang")
+  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 72 87)
 endif()
 set(CUTLASS_NVCC_ARCHS ${CUTLASS_NVCC_ARCHS_SUPPORTED} CACHE STRING "The SM architectures requested.")
 set(CUTLASS_NVCC_ARCHS_ENABLED ${CUTLASS_NVCC_ARCHS} CACHE STRING "The SM architectures to build code for.")
Submodule 3rdparty/flashinfer contains modified content
diff --git a/3rdparty/tvm/3rdparty/flashinfer/include/flashinfer/math.cuh b/3rdparty/tvm/3rdparty/flashinfer/include/flashinfer/math.cuh
index c2401c7..76133a2 100644
--- a/3rdparty/tvm/3rdparty/flashinfer/include/flashinfer/math.cuh
+++ b/3rdparty/tvm/3rdparty/flashinfer/include/flashinfer/math.cuh
@@ -115,13 +115,19 @@ __forceinline__ __device__ float rsqrt(float x) {
 }
 
 /*!
- * \brief Wrapper of PTX tanh.approx.f32 instruction, which computes tanh(x)
+ * \brief Wrapper of PTX tanh.approx.f32 instruction or fallback in SM < 75, which computes tanh(x)
  * \param x input
  */
 __forceinline__ __device__ float tanh(float x) {
+#if (__CUDACC_VER_MAJOR__ < 11) || (__CUDA_ARCH__ < 750)
+  float exp_val = -2.0f * fabsf(x);
+  float e = __expf(exp_val);
+  return copysignf((1.0f - e) / (1.0f + e), x);
+#else
   float y;
   asm volatile("tanh.approx.f32 %0, %1;" : "=f"(y) : "f"(x));
   return y;
+#endif
 }
 
 /*!
@@ -129,10 +135,16 @@ __forceinline__ __device__ float tanh(float x) {
  * \param x input
  */
 __forceinline__ __device__ half2 tanh(half2 x) {
+#if (__CUDACC_VER_MAJOR__ < 11) || (__CUDA_ARCH__ < 750)
+  half lo = __low2half(x);
+  half hi = __high2half(x);
+  return __halves2half2(flashinfer::math::tanh(lo), flashinfer::math::tanh(hi));  // reuse half version
+#else
   uint32_t y_u32;
   uint32_t x_u32 = half2_as_uint32(x);
   asm volatile("tanh.approx.f16x2 %0, %1;" : "=r"(y_u32) : "r"(x_u32));
   return uint32_as_half2(y_u32);
+#endif
 }
 
 /*!
@@ -140,9 +152,15 @@ __forceinline__ __device__ half2 tanh(half2 x) {
  * \param x input
  */
 __forceinline__ __device__ half tanh(half x) {
+#if (__CUDACC_VER_MAJOR__ < 11) || (__CUDA_ARCH__ < 750)
+  float xf = __half2float(x);
+  float yf = flashinfer::math::tanh(xf);  // calls float version
+  return __float2half(yf);
+#else
   ushort y_u16;
   asm volatile("tanh.approx.f16 %0, %1;" : "=h"(y_u16) : "h"(__half_as_ushort(x)));
   return __ushort_as_half(y_u16);
+#endif
 }
 
 }  // namespace math
diff --git a/3rdparty/tvm/3rdparty/flashinfer/python/setup.py b/3rdparty/tvm/3rdparty/flashinfer/python/setup.py
index a23c6ac..3d25a40 100644
--- a/3rdparty/tvm/3rdparty/flashinfer/python/setup.py
+++ b/3rdparty/tvm/3rdparty/flashinfer/python/setup.py
@@ -38,8 +38,8 @@ enable_bf16 = True
 enable_fp8 = True
 for cuda_arch_flags in torch_cpp_ext._get_cuda_arch_flags():
     arch = int(re.search("compute_\d+", cuda_arch_flags).group()[-2:])
-    if arch < 75:
-        raise RuntimeError("FlashInfer requires sm75+")
+    if arch < 72:
+        raise RuntimeError("FlashInfer requires sm72+")
     elif arch == 75:
         # disable bf16 for sm75
         enable_bf16 = False
Submodule 3rdparty/libflash_attn contains modified content
Submodule cutlass contains modified content
diff --git a/3rdparty/tvm/3rdparty/libflash_attn/cutlass/CMakeLists.txt b/3rdparty/tvm/3rdparty/libflash_attn/cutlass/CMakeLists.txt
index 2d4f9cc3..df4212af 100755
--- a/3rdparty/tvm/3rdparty/libflash_attn/cutlass/CMakeLists.txt
+++ b/3rdparty/tvm/3rdparty/libflash_attn/cutlass/CMakeLists.txt
@@ -121,15 +121,7 @@ endif()
 ################################################################################
 
 set(CUTLASS_NVCC_ARCHS_SUPPORTED "")
-if (CUDA_VERSION VERSION_GREATER_EQUAL 11.4 AND NOT CUDA_COMPILER MATCHES "[Cc]lang")
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 70 72 75 80 86 87)
-endif()
-if (CUDA_VERSION VERSION_GREATER_EQUAL 11.8 AND NOT CUDA_COMPILER MATCHES "[Cc]lang")
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 89 90)
-endif()
-if (CUDA_VERSION VERSION_GREATER_EQUAL 12.0 AND NOT CUDA_COMPILER MATCHES "[Cc]lang")
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 90a)
-endif()
+list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 72 87)
 set(CUTLASS_NVCC_ARCHS ${CUTLASS_NVCC_ARCHS_SUPPORTED} CACHE STRING "The SM architectures requested.")
 set(CUTLASS_NVCC_ARCHS_ENABLED ${CUTLASS_NVCC_ARCHS} CACHE STRING "The SM architectures to build code for.")
 
diff --git a/3rdparty/tvm/3rdparty/libflash_attn/cutlass/include/cute/layout_composed.hpp b/3rdparty/tvm/3rdparty/libflash_attn/cutlass/include/cute/layout_composed.hpp
index 7b3b6f4f..3d0e3efe 100644
--- a/3rdparty/tvm/3rdparty/libflash_attn/cutlass/include/cute/layout_composed.hpp
+++ b/3rdparty/tvm/3rdparty/libflash_attn/cutlass/include/cute/layout_composed.hpp
@@ -99,7 +99,9 @@ struct ComposedLayout : private cute::tuple<LayoutA, Offset, LayoutB>  // EBO fo
   // Doesn't really make sense to ask for the strides of this "layout"
   CUTE_HOST_DEVICE constexpr
   decltype(auto)
-  stride() const = delete;
+  stride() const /*= delete*/ {
+    return layout_b().stride();
+  }
 
   //
   // Mappings
@@ -228,7 +230,10 @@ shape(ComposedLayout<A,O,B> const& layout)
 template <int... Is, class Fn, class O, class Layout>
 CUTE_HOST_DEVICE constexpr
 decltype(auto)
-stride(ComposedLayout<Fn,O,Layout> const& layout) = delete;
+stride(ComposedLayout<Fn,O,Layout> const& layout) /*= delete;*/
+{
+  return stride<Is...>(layout.layout_b());
+}
 
 // Return the number of elements in a mode
 template <int... Is, class A, class O, class B>
diff --git a/3rdparty/tvm/3rdparty/libflash_attn/src/CMakeLists.txt b/3rdparty/tvm/3rdparty/libflash_attn/src/CMakeLists.txt
index ba2ac1a..98919bb 100644
--- a/3rdparty/tvm/3rdparty/libflash_attn/src/CMakeLists.txt
+++ b/3rdparty/tvm/3rdparty/libflash_attn/src/CMakeLists.txt
@@ -1,5 +1,6 @@
 set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} --expt-relaxed-constexpr --expt-relaxed-constexpr --use_fast_math -t 8 \
-                      -gencode=arch=compute_80,code=\\\"sm_80,compute_80\\\" \
+                      -gencode=arch=compute_72,code=\\\"sm_72,compute_72\\\" \
+                      -gencode=arch=compute_87,code=\\\"sm_87,compute_87\\\" \
                       ")
 
 include_directories(${CUTLASS_DIR}/include)
@@ -17,4 +18,4 @@ add_library(flash_attn SHARED
   flash_fwd_hdim96_fp16_sm80.cu
 )
 
-set_target_properties(flash_attn PROPERTIES CUDA_ARCHITECTURES "80")
+set_target_properties(flash_attn PROPERTIES CUDA_ARCHITECTURES "72;87")
diff --git a/3rdparty/tvm/3rdparty/libflash_attn/src/static_switch.h b/3rdparty/tvm/3rdparty/libflash_attn/src/static_switch.h
index b4a4b48..6ea899a 100644
--- a/3rdparty/tvm/3rdparty/libflash_attn/src/static_switch.h
+++ b/3rdparty/tvm/3rdparty/libflash_attn/src/static_switch.h
@@ -16,10 +16,10 @@
 #define BOOL_SWITCH(COND, CONST_NAME, ...)                                           \
     [&] {                                                                            \
         if (COND) {                                                                  \
-            constexpr bool CONST_NAME = true;                                        \
+            constexpr static bool CONST_NAME = true;                                        \
             return __VA_ARGS__();                                                    \
         } else {                                                                     \
-            constexpr bool CONST_NAME = false;                                       \
+            constexpr static bool CONST_NAME = false;                                       \
             return __VA_ARGS__();                                                    \
         }                                                                            \
     }()
@@ -38,28 +38,28 @@
 #define FWD_HEADDIM_SWITCH(HEADDIM, ...)  \
     [&] {                                 \
         if (HEADDIM <= 32) {              \
-            constexpr int kHeadDim = 32;  \
+            constexpr static int kHeadDim = 32;  \
             return __VA_ARGS__();         \
         } else if (HEADDIM <= 64) {       \
-            constexpr int kHeadDim = 64;  \
+            constexpr static int kHeadDim = 64;  \
             return __VA_ARGS__();         \
         } else if (HEADDIM <= 96) {       \
-            constexpr int kHeadDim = 96;  \
+            constexpr static int kHeadDim = 96;  \
             return __VA_ARGS__();         \
         } else if (HEADDIM <= 128) {      \
-            constexpr int kHeadDim = 128; \
+            constexpr static int kHeadDim = 128; \
             return __VA_ARGS__();         \
         } else if (HEADDIM <= 160) {      \
-            constexpr int kHeadDim = 160; \
+            constexpr static int kHeadDim = 160; \
             return __VA_ARGS__();         \
         } else if (HEADDIM <= 192) {      \
-            constexpr int kHeadDim = 192; \
+            constexpr static int kHeadDim = 192; \
             return __VA_ARGS__();         \
         } else if (HEADDIM <= 224) {      \
-            constexpr int kHeadDim = 224; \
+            constexpr static int kHeadDim = 224; \
             return __VA_ARGS__();         \
         } else if (HEADDIM <= 256) {      \
-            constexpr int kHeadDim = 256; \
+            constexpr static int kHeadDim = 256; \
             return __VA_ARGS__();         \
         }                                 \
     }()
diff --git a/3rdparty/tvm/CMakeLists.txt b/3rdparty/tvm/CMakeLists.txt
index cf03b71d8..294bd0a0f 100644
--- a/3rdparty/tvm/CMakeLists.txt
+++ b/3rdparty/tvm/CMakeLists.txt
@@ -850,7 +850,7 @@ if(USE_ROCM AND USE_RCCL)
 endif()
 
 
-option(USE_FLASHINFER "Build TVM with FlashInfer" OFF)
+option(USE_FLASHINFER "Build TVM with FlashInfer" ON)
 if (USE_FLASHINFER STREQUAL "ON")
   message(STATUS "Build with FlashInfer")
   set(FLASHINFER_TVM_BINDING ON)
diff --git a/3rdparty/tvm/python/setup.py b/3rdparty/tvm/python/setup.py
index ad0ef6ffd..03eba8dbf 100644
--- a/3rdparty/tvm/python/setup.py
+++ b/3rdparty/tvm/python/setup.py
@@ -127,7 +127,7 @@ def _remove_path(path):
 
 
 LIB_LIST, __version__ = get_lib_path()
-__version__ = git_describe_version(__version__)
+#__version__ = git_describe_version(__version__)
 
 
 def config_cython():
diff --git a/3rdparty/tvm/python/tvm/_ffi/libinfo.py b/3rdparty/tvm/python/tvm/_ffi/libinfo.py
index 3bbc588f4..89a2c39db 100644
--- a/3rdparty/tvm/python/tvm/_ffi/libinfo.py
+++ b/3rdparty/tvm/python/tvm/_ffi/libinfo.py
@@ -247,4 +247,4 @@ def find_include_path(name=None, search_path=None, optional=False):
 # We use the version of the incoming release for code
 # that is under development.
 # The following line is set by tvm/python/update_version.py
-__version__ = "0.20.dev0"
+__version__ = "0.20.0"
diff --git a/3rdparty/tvm/version.py b/3rdparty/tvm/version.py
index c63958b2d..0cfc366dd 100644
--- a/3rdparty/tvm/version.py
+++ b/3rdparty/tvm/version.py
@@ -44,7 +44,7 @@ import subprocess
 # Two tag formats are supported:
 # - vMAJ.MIN.PATCH (e.g. v0.8.0) or
 # - vMAJ.MIN.devN (e.g. v0.8.dev0)
-__version__ = "0.20.dev0"
+__version__ = "0.20.0"
 
 # ---------------------------------------------------
 
diff --git a/CMakeLists.txt b/CMakeLists.txt
index a010a051..68476415 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -47,13 +47,13 @@ set(CMAKE_POSITION_INDEPENDENT_CODE ON)
 # tvm runtime config: minimize runtime components
 set(USE_RPC OFF)
 set(USE_MICRO OFF)
-set(USE_GRAPH_EXECUTOR OFF)
+#set(USE_GRAPH_EXECUTOR OFF)
 set(USE_GRAPH_EXECUTOR_DEBUG OFF)
 set(USE_AOT_EXECUTOR OFF)
 set(USE_PROFILER OFF)
 set(USE_GTEST OFF)
 set(USE_LIBBACKTRACE OFF)
-set(BUILD_DUMMY_LIBTVM ON)
+#set(BUILD_DUMMY_LIBTVM ON)
 if(NOT DEFINED TVM_SOURCE_DIR)
   if(DEFINED ENV{TVM_SOURCE_DIR})
     set(TVM_SOURCE_DIR "$ENV{TVM_SOURCE_DIR}")
diff --git a/python/mlc_llm/libinfo.py b/python/mlc_llm/libinfo.py
index 2212d8c7..a322ef68 100644
--- a/python/mlc_llm/libinfo.py
+++ b/python/mlc_llm/libinfo.py
@@ -4,7 +4,7 @@
 import os
 import sys
 
-__version__ = "0.1.dev0"
+__version__ = "0.20.0"
 MLC_LIBRARY_PATH = os.environ.get("MLC_LIBRARY_PATH", None)
 
 
diff --git a/python/mlc_llm/model/model.py b/python/mlc_llm/model/model.py
index e4b69539..e587c82c 100644
--- a/python/mlc_llm/model/model.py
+++ b/python/mlc_llm/model/model.py
@@ -335,12 +335,14 @@ MODELS: Dict[str, Model] = {
         source={
             "huggingface-torch": qwen3_loader.huggingface,
             "huggingface-safetensor": qwen3_loader.huggingface,
+            "awq": qwen3_loader.awq,
         },
         quantize={
             "no-quant": qwen3_quantization.no_quant,
             "group-quant": qwen3_quantization.group_quant,
             "ft-quant": qwen3_quantization.ft_quant,
             "block-scale-quant": qwen3_quantization.block_scale_quant,
+            "awq": qwen3_quantization.awq_quant,
         },
     ),
     "qwen3_moe": Model(
diff --git a/python/mlc_llm/model/qwen3/qwen3_loader.py b/python/mlc_llm/model/qwen3/qwen3_loader.py
index 88cdaebd..a3b0bdb3 100644
--- a/python/mlc_llm/model/qwen3/qwen3_loader.py
+++ b/python/mlc_llm/model/qwen3/qwen3_loader.py
@@ -12,6 +12,7 @@ from mlc_llm.loader import ExternMapping, QuantizeMapping
 from mlc_llm.quantization import BlockScaleQuantize, Quantization
 
 from .qwen3_model import Qwen3Config, Qwen3LMHeadModel
+from .qwen3_quantization import awq_quant
 
 
 def huggingface(model_config: Qwen3Config, quantization: Quantization) -> ExternMapping:
@@ -133,3 +134,84 @@ def huggingface(model_config: Qwen3Config, quantization: Quantization) -> Extern
                 ),
             )
     return mapping
+
+def awq(model_config: Qwen3Config, quantization: Quantization) -> ExternMapping:
+    """Returns a parameter mapping that maps from the names of MLC LLM parameters to
+    the names of AWQ parameters.
+    Parameters
+    ----------
+    model_config : Qwen3Config
+        The configuration of the Qwen3 model.
+
+    quantization : Quantization
+        The quantization configuration.
+
+    Returns
+    -------
+    param_map : ExternMapping
+        The parameter mapping from MLC to AWQ.
+    """
+    model, _ = awq_quant(model_config, quantization)
+    _, _named_params, _ = model.export_tvm(  # type: ignore[misc]
+        spec=model.get_default_spec(),  # type: ignore[attr-defined]
+        allow_extern=True,
+    )
+    named_parameters = dict(_named_params)
+
+    mapping = ExternMapping()
+
+    for i in range(model_config.num_hidden_layers):
+        # map attention weight
+        attn = f"model.layers.{i}.self_attn"
+        for quantize_suffix in ["qweight", "qzeros", "scales"]:
+            mlc_name = f"{attn}.c_attn.{quantize_suffix}"
+            assert mlc_name in named_parameters
+            mlc_param = named_parameters[mlc_name]
+            mapping.add_mapping(
+                mlc_name,
+                [
+                    f"{attn}.q_proj.{quantize_suffix}",
+                    f"{attn}.k_proj.{quantize_suffix}",
+                    f"{attn}.v_proj.{quantize_suffix}",
+                ],
+                functools.partial(
+                    lambda q, k, v, dtype: np.concatenate(
+                        [q, k, v],
+                        axis=1,  # AWQ GEMM would transpose the weight
+                    ).astype(dtype),
+                    dtype=mlc_param.dtype,
+                ),
+            )
+
+        # Concat gate and up in MLP
+        mlp = f"model.layers.{i}.mlp"
+        for quantize_suffix in ["qweight", "qzeros", "scales"]:
+            mlc_name = f"{mlp}.gate_up_proj.{quantize_suffix}"
+            assert mlc_name in named_parameters
+            mlc_param = named_parameters[mlc_name]
+            mapping.add_mapping(
+                mlc_name,
+                [
+                    f"{mlp}.gate_proj.{quantize_suffix}",
+                    f"{mlp}.up_proj.{quantize_suffix}",
+                ],
+                functools.partial(
+                    lambda gate, up, dtype: np.concatenate(
+                        [gate, up],
+                        axis=1,  # AWQ GEMM would transpose the weight
+                    ).astype(dtype),
+                    dtype=mlc_param.dtype,
+                ),
+            )
+
+        # inv_freq is not used in the model
+        mapping.add_unused(f"{attn}.rotary_emb.inv_freq")
+
+    for mlc_name, mlc_param in named_parameters.items():
+        if mlc_name not in mapping.param_map:
+            mapping.add_mapping(
+                mlc_name,
+                [mlc_name],
+                functools.partial(lambda x, dtype: x.astype(dtype), dtype=mlc_param.dtype),
+            )
+    return mapping
diff --git a/python/mlc_llm/model/qwen3/qwen3_model.py b/python/mlc_llm/model/qwen3/qwen3_model.py
index a4468ffe..244368af 100644
--- a/python/mlc_llm/model/qwen3/qwen3_model.py
+++ b/python/mlc_llm/model/qwen3/qwen3_model.py
@@ -59,13 +59,13 @@ class Qwen3Config(ConfigBase):  # pylint: disable=too-many-instance-attributes
                     not isinstance(self.weight_block_size, (tuple, list))
                     or len(self.weight_block_size) != 2
                 ):
-                    raise ValueError(
+                    logger.info(
                         "Invalid DeepSeek model quantization config: "
                         "weight_block_size must be a tuple of two integers, "
                         f"got {self.weight_block_size} of type {type(self.weight_block_size)}"
                     )
             else:
-                raise ValueError(
+                logger.info(
                     "Invalid DeepSeek model quantization config: unrecognized quantization config: "
                     f"{quantization_config}"
                 )
diff --git a/python/mlc_llm/model/qwen3/qwen3_quantization.py b/python/mlc_llm/model/qwen3/qwen3_quantization.py
index 2f3a9904..4d940246 100644
--- a/python/mlc_llm/model/qwen3/qwen3_quantization.py
+++ b/python/mlc_llm/model/qwen3/qwen3_quantization.py
@@ -7,6 +7,7 @@ from tvm.relax.frontend import nn
 
 from mlc_llm.loader import QuantizeMapping
 from mlc_llm.quantization import (
+    AWQQuantize,
     BlockScaleQuantize,
     FTQuantize,
     GroupQuantize,
@@ -49,6 +50,22 @@ def ft_quant(
     return model, quant_map
 
 
+def awq_quant(
+    model_config: Qwen3Config,
+    quantization: AWQQuantize,
+) -> Tuple[nn.Module, QuantizeMapping]:
+    """Quantize a Llama-architecture model using Activation-aware Weight Quantization(AWQ)."""
+    model: nn.Module = Qwen3LMHeadModel(model_config)
+    model.to(quantization.model_dtype)
+    quant_map = QuantizeMapping({}, {})
+    model = quantization.quantize_model(
+        model,
+        quant_map,
+        "",
+    )
+    return model, quant_map
+
+
 def no_quant(
     model_config: Qwen3Config,
     quantization: NoQuantize,
diff --git a/python/mlc_llm/quantization/ft_quantization.py b/python/mlc_llm/quantization/ft_quantization.py
index 4a158460..7cd8fc6a 100644
--- a/python/mlc_llm/quantization/ft_quantization.py
+++ b/python/mlc_llm/quantization/ft_quantization.py
@@ -207,7 +207,7 @@ class FTQuantize:  # pylint: disable=too-many-instance-attributes
                                 relax.call_pure_packed(
                                     "cutlass.ft_preprocess_weight",
                                     lv1,
-                                    detect_cuda_arch_list(target=target)[0],
+                                    int(detect_cuda_arch_list(target=target)[0]),
                                     DataType(self.quantize_dtype).bits == 4,
                                     sinfo_args=lv1.struct_info,
                                 )
diff --git a/python/setup.py b/python/setup.py
index 0eb7a3a7..b964530e 100644
--- a/python/setup.py
+++ b/python/setup.py
@@ -47,7 +47,7 @@ def git_describe_version(original_version):
 
 
 LIB_LIST, __version__ = get_lib_path()
-__version__ = git_describe_version(__version__)
+#__version__ = git_describe_version(__version__)
 
 
 class BinaryDistribution(Distribution):
diff --git a/version.py b/version.py
index c7868f84..fe4ba514 100644
--- a/version.py
+++ b/version.py
@@ -20,7 +20,7 @@ import subprocess
 
 # ---------------------------------------------------
 
-__version__ = "0.1.dev0"
+__version__ = "0.20.0"
 PROJ_ROOT = os.path.dirname(os.path.abspath(os.path.expanduser(__file__)))
 
 
