Submodule 3rdparty/tvm contains modified content
Submodule 3rdparty/cutlass contains modified content
diff --git a/3rdparty/tvm/3rdparty/cutlass/CMakeLists.txt b/3rdparty/tvm/3rdparty/cutlass/CMakeLists.txt
index b54b8335..d31cf70a 100755
--- a/3rdparty/tvm/3rdparty/cutlass/CMakeLists.txt
+++ b/3rdparty/tvm/3rdparty/cutlass/CMakeLists.txt
@@ -165,17 +165,7 @@ set(CUTLASS_ENABLE_SELF_CONTAINED_INCLUDES_CHECK ON CACHE BOOL "Enable CUTLASS c
 
 set(CUTLASS_NVCC_ARCHS_SUPPORTED "")
 if (CUDA_VERSION VERSION_GREATER_EQUAL 11.4)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 70 72 75 80 86 87)
-endif()
-if (CUDA_VERSION VERSION_GREATER_EQUAL 11.8)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 89 90)
-endif()
-if (CUDA_VERSION VERSION_GREATER_EQUAL 12.0)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 90a)
-endif()
-
-if (CUDA_VERSION VERSION_GREATER_EQUAL 12.8)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 100 100a 101 101a 120 120a)
+  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 72 87)
 endif()
 
 set(CUTLASS_NVCC_ARCHS ${CUTLASS_NVCC_ARCHS_SUPPORTED} CACHE STRING "The SM architectures requested.")
Submodule 3rdparty/cutlass_fpA_intB_gemm contains modified content
Submodule cutlass contains modified content
diff --git a/3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm/cutlass/CMakeLists.txt b/3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm/cutlass/CMakeLists.txt
index 30e261c2..bb5f2427 100755
--- a/3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm/cutlass/CMakeLists.txt
+++ b/3rdparty/tvm/3rdparty/cutlass_fpA_intB_gemm/cutlass/CMakeLists.txt
@@ -101,26 +101,8 @@ if (CUTLASS_ENABLE_TESTS)
 endif()
 
 set(CUTLASS_NVCC_ARCHS_SUPPORTED "")
-if (NOT CUDA_VERSION VERSION_LESS 7.5)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 53)
-endif()
-if (NOT CUDA_VERSION VERSION_LESS 8.0)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 60 61)
-endif()
-if (NOT CUDA_VERSION VERSION_LESS 9.0)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 70)
-endif()
-if (NOT CUDA_VERSION VERSION_LESS 9.2)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 72)
-endif()
-if (NOT CUDA_VERSION VERSION_LESS 10.0)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 75)
-endif()
-if (NOT CUDA_VERSION VERSION_LESS 11.0)
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 80)
-endif()
-if (NOT CUDA_VERSION VERSION_LESS 11.1 AND NOT CUDA_COMPILER MATCHES "[Cc]lang")
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 86)
+if (CUDA_VERSION VERSION_GREATER_EQUAL 11.4 AND NOT CUDA_COMPILER MATCHES "[Cc]lang")
+  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 72 87)
 endif()
 set(CUTLASS_NVCC_ARCHS ${CUTLASS_NVCC_ARCHS_SUPPORTED} CACHE STRING "The SM architectures requested.")
 set(CUTLASS_NVCC_ARCHS_ENABLED ${CUTLASS_NVCC_ARCHS} CACHE STRING "The SM architectures to build code for.")
Submodule 3rdparty/libflash_attn contains modified content
Submodule cutlass contains modified content
diff --git a/3rdparty/tvm/3rdparty/libflash_attn/cutlass/CMakeLists.txt b/3rdparty/tvm/3rdparty/libflash_attn/cutlass/CMakeLists.txt
index 2d4f9cc3..726751ec 100755
--- a/3rdparty/tvm/3rdparty/libflash_attn/cutlass/CMakeLists.txt
+++ b/3rdparty/tvm/3rdparty/libflash_attn/cutlass/CMakeLists.txt
@@ -122,13 +122,7 @@ endif()
 
 set(CUTLASS_NVCC_ARCHS_SUPPORTED "")
 if (CUDA_VERSION VERSION_GREATER_EQUAL 11.4 AND NOT CUDA_COMPILER MATCHES "[Cc]lang")
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 70 72 75 80 86 87)
-endif()
-if (CUDA_VERSION VERSION_GREATER_EQUAL 11.8 AND NOT CUDA_COMPILER MATCHES "[Cc]lang")
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 89 90)
-endif()
-if (CUDA_VERSION VERSION_GREATER_EQUAL 12.0 AND NOT CUDA_COMPILER MATCHES "[Cc]lang")
-  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 90a)
+  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 72 87)
 endif()
 set(CUTLASS_NVCC_ARCHS ${CUTLASS_NVCC_ARCHS_SUPPORTED} CACHE STRING "The SM architectures requested.")
 set(CUTLASS_NVCC_ARCHS_ENABLED ${CUTLASS_NVCC_ARCHS} CACHE STRING "The SM architectures to build code for.")
diff --git a/3rdparty/tvm/3rdparty/libflash_attn/cutlass/include/cute/layout_composed.hpp b/3rdparty/tvm/3rdparty/libflash_attn/cutlass/include/cute/layout_composed.hpp
index 7b3b6f4f..3d0e3efe 100644
--- a/3rdparty/tvm/3rdparty/libflash_attn/cutlass/include/cute/layout_composed.hpp
+++ b/3rdparty/tvm/3rdparty/libflash_attn/cutlass/include/cute/layout_composed.hpp
@@ -99,7 +99,9 @@ struct ComposedLayout : private cute::tuple<LayoutA, Offset, LayoutB>  // EBO fo
   // Doesn't really make sense to ask for the strides of this "layout"
   CUTE_HOST_DEVICE constexpr
   decltype(auto)
-  stride() const = delete;
+  stride() const /*= delete*/ {
+    return layout_b().stride();
+  }
 
   //
   // Mappings
@@ -228,7 +230,10 @@ shape(ComposedLayout<A,O,B> const& layout)
 template <int... Is, class Fn, class O, class Layout>
 CUTE_HOST_DEVICE constexpr
 decltype(auto)
-stride(ComposedLayout<Fn,O,Layout> const& layout) = delete;
+stride(ComposedLayout<Fn,O,Layout> const& layout) /*= delete;*/
+{
+  return stride<Is...>(layout.layout_b());
+}
 
 // Return the number of elements in a mode
 template <int... Is, class A, class O, class B>
diff --git a/3rdparty/tvm/3rdparty/libflash_attn/src/CMakeLists.txt b/3rdparty/tvm/3rdparty/libflash_attn/src/CMakeLists.txt
index ba2ac1a..98919bb 100644
--- a/3rdparty/tvm/3rdparty/libflash_attn/src/CMakeLists.txt
+++ b/3rdparty/tvm/3rdparty/libflash_attn/src/CMakeLists.txt
@@ -1,5 +1,6 @@
 set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} --expt-relaxed-constexpr --expt-relaxed-constexpr --use_fast_math -t 8 \
-                      -gencode=arch=compute_80,code=\\\"sm_80,compute_80\\\" \
+                      -gencode=arch=compute_72,code=\\\"sm_72,compute_72\\\" \
+                      -gencode=arch=compute_87,code=\\\"sm_87,compute_87\\\" \
                       ")
 
 include_directories(${CUTLASS_DIR}/include)
@@ -17,4 +18,4 @@ add_library(flash_attn SHARED
   flash_fwd_hdim96_fp16_sm80.cu
 )
 
-set_target_properties(flash_attn PROPERTIES CUDA_ARCHITECTURES "80")
+set_target_properties(flash_attn PROPERTIES CUDA_ARCHITECTURES "72;87")
diff --git a/3rdparty/tvm/3rdparty/libflash_attn/src/static_switch.h b/3rdparty/tvm/3rdparty/libflash_attn/src/static_switch.h
index b4a4b48..2ce710f 100644
--- a/3rdparty/tvm/3rdparty/libflash_attn/src/static_switch.h
+++ b/3rdparty/tvm/3rdparty/libflash_attn/src/static_switch.h
@@ -4,7 +4,7 @@
 #pragma once
 
 /// @param COND       - a boolean expression to switch by
-/// @param CONST_NAME - a name given for the constexpr bool variable.
+/// @param CONST_NAME - a name given for the constexpr static bool variable.
 /// @param ...       - code to execute for true and false
 ///
 /// Usage:
@@ -16,10 +16,10 @@
 #define BOOL_SWITCH(COND, CONST_NAME, ...)                                           \
     [&] {                                                                            \
         if (COND) {                                                                  \
-            constexpr bool CONST_NAME = true;                                        \
+            constexpr static bool CONST_NAME = true;                                        \
             return __VA_ARGS__();                                                    \
         } else {                                                                     \
-            constexpr bool CONST_NAME = false;                                       \
+            constexpr static bool CONST_NAME = false;                                       \
             return __VA_ARGS__();                                                    \
         }                                                                            \
     }()
@@ -38,28 +38,28 @@
 #define FWD_HEADDIM_SWITCH(HEADDIM, ...)  \
     [&] {                                 \
         if (HEADDIM <= 32) {              \
-            constexpr int kHeadDim = 32;  \
+            constexpr static int kHeadDim = 32;  \
             return __VA_ARGS__();         \
         } else if (HEADDIM <= 64) {       \
-            constexpr int kHeadDim = 64;  \
+            constexpr static int kHeadDim = 64;  \
             return __VA_ARGS__();         \
         } else if (HEADDIM <= 96) {       \
-            constexpr int kHeadDim = 96;  \
+            constexpr static int kHeadDim = 96;  \
             return __VA_ARGS__();         \
         } else if (HEADDIM <= 128) {      \
-            constexpr int kHeadDim = 128; \
+            constexpr static int kHeadDim = 128; \
             return __VA_ARGS__();         \
         } else if (HEADDIM <= 160) {      \
-            constexpr int kHeadDim = 160; \
+            constexpr static int kHeadDim = 160; \
             return __VA_ARGS__();         \
         } else if (HEADDIM <= 192) {      \
-            constexpr int kHeadDim = 192; \
+            constexpr static int kHeadDim = 192; \
             return __VA_ARGS__();         \
         } else if (HEADDIM <= 224) {      \
-            constexpr int kHeadDim = 224; \
+            constexpr static int kHeadDim = 224; \
             return __VA_ARGS__();         \
         } else if (HEADDIM <= 256) {      \
-            constexpr int kHeadDim = 256; \
+            constexpr static int kHeadDim = 256; \
             return __VA_ARGS__();         \
         }                                 \
     }()
diff --git a/3rdparty/tvm/cmake/config.cmake b/3rdparty/tvm/cmake/config.cmake
index 7571a1766..b1514f359 100644
--- a/3rdparty/tvm/cmake/config.cmake
+++ b/3rdparty/tvm/cmake/config.cmake
@@ -395,7 +395,7 @@ set(USE_GTEST AUTO)
 set(USE_CUTLASS OFF)
 
 # Whether to enable FlashInfer or not.
-set(USE_FLASHINFER OFF)
+set(USE_FLASHINFER ON)
 # Options for FlashInfer kernel compilation.
 set(FLASHINFER_ENABLE_FP8 OFF)
 set(FLASHINFER_ENABLE_BF16 OFF)
diff --git a/3rdparty/tvm/conda/recipe/meta.yaml b/3rdparty/tvm/conda/recipe/meta.yaml
index d9b1db497..ddc2570ef 100644
--- a/3rdparty/tvm/conda/recipe/meta.yaml
+++ b/3rdparty/tvm/conda/recipe/meta.yaml
@@ -15,7 +15,7 @@
 # specific language governing permissions and limitations
 # under the License.
 
-{% set version = '0.21.dev0' %}
+{% set version = '0.21.0' %}
 {% set pkg_name = 'tvm' %}
 {% set cuda_tag = cuda_version | replace('.', '') %} # [cuda]
 {% set pkg_name = pkg_name + '-cu' + cuda_tag %} # [cuda]
diff --git a/3rdparty/tvm/include/tvm/runtime/base.h b/3rdparty/tvm/include/tvm/runtime/base.h
index c704decb6..31404206a 100644
--- a/3rdparty/tvm/include/tvm/runtime/base.h
+++ b/3rdparty/tvm/include/tvm/runtime/base.h
@@ -29,7 +29,7 @@
 #include <tvm/ffi/c_api.h>
 
 // TVM version
-#define TVM_VERSION "0.21.dev0"
+#define TVM_VERSION "0.21.0"
 
 // define extra macros for TVM DLL exprt
 #ifdef __EMSCRIPTEN__
diff --git a/3rdparty/tvm/python/setup.py b/3rdparty/tvm/python/setup.py
index 679f5078d..ce4f80ab3 100644
--- a/3rdparty/tvm/python/setup.py
+++ b/3rdparty/tvm/python/setup.py
@@ -128,7 +128,7 @@ def _remove_path(path):
 
 
 LIB_LIST, __version__ = get_lib_path()
-__version__ = git_describe_version(__version__)
+# __version__ = git_describe_version(__version__)
 
 if not CONDA_BUILD and not INPLACE_BUILD:
     # Wheel cleanup
diff --git a/3rdparty/tvm/python/tvm/libinfo.py b/3rdparty/tvm/python/tvm/libinfo.py
index d05f44854..316e5e286 100644
--- a/3rdparty/tvm/python/tvm/libinfo.py
+++ b/3rdparty/tvm/python/tvm/libinfo.py
@@ -259,4 +259,4 @@ def find_include_path(name=None, search_path=None, optional=False):
 # We use the version of the incoming release for code
 # that is under development.
 # The following line is set by tvm/python/update_version.py
-__version__ = "0.21.dev0"
+__version__ = "0.21.0"
diff --git a/3rdparty/tvm/version.py b/3rdparty/tvm/version.py
index bf2f0343d..d331ee65a 100644
--- a/3rdparty/tvm/version.py
+++ b/3rdparty/tvm/version.py
@@ -44,7 +44,7 @@ import subprocess
 # Two tag formats are supported:
 # - vMAJ.MIN.PATCH (e.g. v0.8.0) or
 # - vMAJ.MIN.devN (e.g. v0.8.dev0)
-__version__ = "0.21.dev0"
+__version__ = "0.21.0"
 
 # ---------------------------------------------------
 
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 837b6e8b..9f415754 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -47,13 +47,13 @@ set(CMAKE_POSITION_INDEPENDENT_CODE ON)
 # tvm runtime config: minimize runtime components
 set(USE_RPC OFF)
 set(USE_MICRO OFF)
-set(USE_GRAPH_EXECUTOR OFF)
+#set(USE_GRAPH_EXECUTOR OFF)
 set(USE_GRAPH_EXECUTOR_DEBUG OFF)
 set(USE_AOT_EXECUTOR OFF)
 set(USE_PROFILER OFF)
 set(USE_GTEST OFF)
 set(USE_LIBBACKTRACE OFF)
-set(BUILD_DUMMY_LIBTVM ON)
+#set(BUILD_DUMMY_LIBTVM ON)
 if(NOT DEFINED TVM_SOURCE_DIR)
   if(DEFINED ENV{TVM_SOURCE_DIR})
     set(TVM_SOURCE_DIR "$ENV{TVM_SOURCE_DIR}")
diff --git a/python/mlc_llm/libinfo.py b/python/mlc_llm/libinfo.py
index 2212d8c7..92ce8b92 100644
--- a/python/mlc_llm/libinfo.py
+++ b/python/mlc_llm/libinfo.py
@@ -4,7 +4,7 @@
 import os
 import sys
 
-__version__ = "0.1.dev0"
+__version__ = "0.21.0"
 MLC_LIBRARY_PATH = os.environ.get("MLC_LIBRARY_PATH", None)
 
 
diff --git a/python/mlc_llm/model/model.py b/python/mlc_llm/model/model.py
index e4b69539..e587c82c 100644
--- a/python/mlc_llm/model/model.py
+++ b/python/mlc_llm/model/model.py
@@ -335,12 +335,14 @@ MODELS: Dict[str, Model] = {
         source={
             "huggingface-torch": qwen3_loader.huggingface,
             "huggingface-safetensor": qwen3_loader.huggingface,
+            "awq": qwen3_loader.awq,
         },
         quantize={
             "no-quant": qwen3_quantization.no_quant,
             "group-quant": qwen3_quantization.group_quant,
             "ft-quant": qwen3_quantization.ft_quant,
             "block-scale-quant": qwen3_quantization.block_scale_quant,
+            "awq": qwen3_quantization.awq_quant,
         },
     ),
     "qwen3_moe": Model(
diff --git a/python/mlc_llm/model/qwen3/qwen3_loader.py b/python/mlc_llm/model/qwen3/qwen3_loader.py
index 88cdaebd..a3b0bdb3 100644
--- a/python/mlc_llm/model/qwen3/qwen3_loader.py
+++ b/python/mlc_llm/model/qwen3/qwen3_loader.py
@@ -12,6 +12,7 @@ from mlc_llm.loader import ExternMapping, QuantizeMapping
 from mlc_llm.quantization import BlockScaleQuantize, Quantization
 
 from .qwen3_model import Qwen3Config, Qwen3LMHeadModel
+from .qwen3_quantization import awq_quant
 
 
 def huggingface(model_config: Qwen3Config, quantization: Quantization) -> ExternMapping:
@@ -133,3 +134,84 @@ def huggingface(model_config: Qwen3Config, quantization: Quantization) -> Extern
                 ),
             )
     return mapping
+
+def awq(model_config: Qwen3Config, quantization: Quantization) -> ExternMapping:
+    """Returns a parameter mapping that maps from the names of MLC LLM parameters to
+    the names of AWQ parameters.
+    Parameters
+    ----------
+    model_config : Qwen3Config
+        The configuration of the Qwen3 model.
+
+    quantization : Quantization
+        The quantization configuration.
+
+    Returns
+    -------
+    param_map : ExternMapping
+        The parameter mapping from MLC to AWQ.
+    """
+    model, _ = awq_quant(model_config, quantization)
+    _, _named_params, _ = model.export_tvm(  # type: ignore[misc]
+        spec=model.get_default_spec(),  # type: ignore[attr-defined]
+        allow_extern=True,
+    )
+    named_parameters = dict(_named_params)
+
+    mapping = ExternMapping()
+
+    for i in range(model_config.num_hidden_layers):
+        # map attention weight
+        attn = f"model.layers.{i}.self_attn"
+        for quantize_suffix in ["qweight", "qzeros", "scales"]:
+            mlc_name = f"{attn}.c_attn.{quantize_suffix}"
+            assert mlc_name in named_parameters
+            mlc_param = named_parameters[mlc_name]
+            mapping.add_mapping(
+                mlc_name,
+                [
+                    f"{attn}.q_proj.{quantize_suffix}",
+                    f"{attn}.k_proj.{quantize_suffix}",
+                    f"{attn}.v_proj.{quantize_suffix}",
+                ],
+                functools.partial(
+                    lambda q, k, v, dtype: np.concatenate(
+                        [q, k, v],
+                        axis=1,  # AWQ GEMM would transpose the weight
+                    ).astype(dtype),
+                    dtype=mlc_param.dtype,
+                ),
+            )
+
+        # Concat gate and up in MLP
+        mlp = f"model.layers.{i}.mlp"
+        for quantize_suffix in ["qweight", "qzeros", "scales"]:
+            mlc_name = f"{mlp}.gate_up_proj.{quantize_suffix}"
+            assert mlc_name in named_parameters
+            mlc_param = named_parameters[mlc_name]
+            mapping.add_mapping(
+                mlc_name,
+                [
+                    f"{mlp}.gate_proj.{quantize_suffix}",
+                    f"{mlp}.up_proj.{quantize_suffix}",
+                ],
+                functools.partial(
+                    lambda gate, up, dtype: np.concatenate(
+                        [gate, up],
+                        axis=1,  # AWQ GEMM would transpose the weight
+                    ).astype(dtype),
+                    dtype=mlc_param.dtype,
+                ),
+            )
+
+        # inv_freq is not used in the model
+        mapping.add_unused(f"{attn}.rotary_emb.inv_freq")
+
+    for mlc_name, mlc_param in named_parameters.items():
+        if mlc_name not in mapping.param_map:
+            mapping.add_mapping(
+                mlc_name,
+                [mlc_name],
+                functools.partial(lambda x, dtype: x.astype(dtype), dtype=mlc_param.dtype),
+            )
+    return mapping
diff --git a/python/mlc_llm/model/qwen3/qwen3_model.py b/python/mlc_llm/model/qwen3/qwen3_model.py
index a4468ffe..244368af 100644
--- a/python/mlc_llm/model/qwen3/qwen3_model.py
+++ b/python/mlc_llm/model/qwen3/qwen3_model.py
@@ -59,13 +59,13 @@ class Qwen3Config(ConfigBase):  # pylint: disable=too-many-instance-attributes
                     not isinstance(self.weight_block_size, (tuple, list))
                     or len(self.weight_block_size) != 2
                 ):
-                    raise ValueError(
+                    logger.info(
                         "Invalid DeepSeek model quantization config: "
                         "weight_block_size must be a tuple of two integers, "
                         f"got {self.weight_block_size} of type {type(self.weight_block_size)}"
                     )
             else:
-                raise ValueError(
+                logger.info(
                     "Invalid DeepSeek model quantization config: unrecognized quantization config: "
                     f"{quantization_config}"
                 )
diff --git a/python/mlc_llm/model/qwen3/qwen3_quantization.py b/python/mlc_llm/model/qwen3/qwen3_quantization.py
index 2f3a9904..4d940246 100644
--- a/python/mlc_llm/model/qwen3/qwen3_quantization.py
+++ b/python/mlc_llm/model/qwen3/qwen3_quantization.py
@@ -7,6 +7,7 @@ from tvm.relax.frontend import nn
 
 from mlc_llm.loader import QuantizeMapping
 from mlc_llm.quantization import (
+    AWQQuantize,
     BlockScaleQuantize,
     FTQuantize,
     GroupQuantize,
@@ -49,6 +50,22 @@ def ft_quant(
     return model, quant_map
 
 
+def awq_quant(
+    model_config: Qwen3Config,
+    quantization: AWQQuantize,
+) -> Tuple[nn.Module, QuantizeMapping]:
+    """Quantize a Llama-architecture model using Activation-aware Weight Quantization(AWQ)."""
+    model: nn.Module = Qwen3LMHeadModel(model_config)
+    model.to(quantization.model_dtype)
+    quant_map = QuantizeMapping({}, {})
+    model = quantization.quantize_model(
+        model,
+        quant_map,
+        "",
+    )
+    return model, quant_map
+
+
 def no_quant(
     model_config: Qwen3Config,
     quantization: NoQuantize,
diff --git a/python/mlc_llm/quantization/ft_quantization.py b/python/mlc_llm/quantization/ft_quantization.py
index d15052ce..a6fc36dd 100644
--- a/python/mlc_llm/quantization/ft_quantization.py
+++ b/python/mlc_llm/quantization/ft_quantization.py
@@ -207,7 +207,7 @@ class FTQuantize:  # pylint: disable=too-many-instance-attributes
                                 relax.call_pure_packed(
                                     "cutlass.ft_preprocess_weight",
                                     lv1,
-                                    detect_cuda_arch_list(target=target)[0],
+                                    int(detect_cuda_arch_list(target=target)[0]),
                                     DataType(self.quantize_dtype).bits == 4,
                                     sinfo_args=lv1.struct_info,
                                 )
diff --git a/python/setup.py b/python/setup.py
index 0eb7a3a7..ab03329a 100644
--- a/python/setup.py
+++ b/python/setup.py
@@ -47,7 +47,7 @@ def git_describe_version(original_version):
 
 
 LIB_LIST, __version__ = get_lib_path()
-__version__ = git_describe_version(__version__)
+# __version__ = git_describe_version(__version__)
 
 
 class BinaryDistribution(Distribution):
diff --git a/version.py b/version.py
index c7868f84..babadec3 100644
--- a/version.py
+++ b/version.py
@@ -20,7 +20,7 @@ import subprocess
 
 # ---------------------------------------------------
 
-__version__ = "0.1.dev0"
+__version__ = "0.21.0"
 PROJ_ROOT = os.path.dirname(os.path.abspath(os.path.expanduser(__file__)))
 
 
