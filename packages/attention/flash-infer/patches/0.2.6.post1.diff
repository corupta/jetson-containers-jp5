diff --git a/flashinfer/jit/core.py b/flashinfer/jit/core.py
index 83931ed..070ef88 100644
--- a/flashinfer/jit/core.py
+++ b/flashinfer/jit/core.py
@@ -48,8 +48,8 @@ def check_cuda_arch():
     # cuda arch check for fp8 at the moment.
     for cuda_arch_flags in torch_cpp_ext._get_cuda_arch_flags():
         arch = int(re.search(r"compute_(\d+)", cuda_arch_flags).group(1))
-        if arch < 75:
-            raise RuntimeError("FlashInfer requires sm75+")
+        if arch < 72:
+            raise RuntimeError("FlashInfer requires sm72+")
 
 
 def clear_cache_dir():
diff --git a/include/flashinfer/math.cuh b/include/flashinfer/math.cuh
index 27c6351..560f512 100644
--- a/include/flashinfer/math.cuh
+++ b/include/flashinfer/math.cuh
@@ -60,10 +60,16 @@ __forceinline__ __device__ float ptx_log2(float x) {
  * \param x input
  */
 __forceinline__ __device__ half2 ptx_exp2(half2 x) {
+#if (__CUDACC_VER_MAJOR__ < 11) || (__CUDA_ARCH__ < 750)
+  half lo = __low2half(x);
+  half hi = __high2half(x);
+  return __halves2half2(flashinfer::math::ptx_exp2(lo), flashinfer::math::ptx_exp2(hi));  // reuse half version
+#else
   uint32_t y_u32;
   uint32_t x_u32 = half2_as_uint32(x);
   asm volatile("ex2.approx.f16x2 %0, %1;" : "=r"(y_u32) : "r"(x_u32));
   return uint32_as_half2(y_u32);
+#endif
 }
 
 /*!
@@ -71,9 +77,15 @@ __forceinline__ __device__ half2 ptx_exp2(half2 x) {
  * \param x input
  */
 __forceinline__ __device__ half ptx_exp2(half x) {
+#if (__CUDACC_VER_MAJOR__ < 11) || (__CUDA_ARCH__ < 750)
+  float xf = __half2float(x);
+  float yf = flashinfer::math::ptx_exp2(xf);  // calls float version
+  return __float2half(yf);
+#else
   ushort y_u16;
   asm volatile("ex2.approx.f16 %0, %1;" : "=h"(y_u16) : "h"(__half_as_ushort(x)));
   return __ushort_as_half(y_u16);
+#endif
 }
 
 /*!
@@ -121,13 +133,19 @@ __forceinline__ __device__ float rsqrt(float x) {
 }
 
 /*!
- * \brief Wrapper of PTX tanh.approx.f32 instruction, which computes tanh(x)
+ * \brief Wrapper of PTX tanh.approx.f32 instruction or fallback in SM < 75, which computes tanh(x)
  * \param x input
  */
 __forceinline__ __device__ float tanh(float x) {
+#if (__CUDACC_VER_MAJOR__ < 11) || (__CUDA_ARCH__ < 750)
+  float exp_val = -2.0f * fabsf(x);
+  float e = __expf(exp_val);
+  return copysignf((1.0f - e) / (1.0f + e), x);
+#else
   float y;
   asm volatile("tanh.approx.f32 %0, %1;" : "=f"(y) : "f"(x));
   return y;
+#endif
 }
 
 /*!
@@ -135,10 +153,16 @@ __forceinline__ __device__ float tanh(float x) {
  * \param x input
  */
 __forceinline__ __device__ half2 tanh(half2 x) {
+#if (__CUDACC_VER_MAJOR__ < 11) || (__CUDA_ARCH__ < 750)
+  half lo = __low2half(x);
+  half hi = __high2half(x);
+  return __halves2half2(flashinfer::math::tanh(lo), flashinfer::math::tanh(hi));  // reuse half version
+#else
   uint32_t y_u32;
   uint32_t x_u32 = half2_as_uint32(x);
   asm volatile("tanh.approx.f16x2 %0, %1;" : "=r"(y_u32) : "r"(x_u32));
   return uint32_as_half2(y_u32);
+#endif
 }
 
 /*!
@@ -146,9 +170,15 @@ __forceinline__ __device__ half2 tanh(half2 x) {
  * \param x input
  */
 __forceinline__ __device__ half tanh(half x) {
+#if (__CUDACC_VER_MAJOR__ < 11) || (__CUDA_ARCH__ < 750)
+  float xf = __half2float(x);
+  float yf = flashinfer::math::tanh(xf);  // calls float version
+  return __float2half(yf);
+#else
   ushort y_u16;
   asm volatile("tanh.approx.f16 %0, %1;" : "=h"(y_u16) : "h"(__half_as_ushort(x)));
   return __ushort_as_half(y_u16);
+#endif
 }
 
 }  // namespace math
